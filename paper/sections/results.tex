% Results Section

We present our main results on AlphaResearchComp, followed by detailed ablation analysis.

%------------------------------------------------------------------------------
\subsection{Main Results}
\label{sec:main_results}

Table~\ref{tab:main_results} presents the performance of \ours{} compared to baselines on all 8 benchmark problems.

\begin{table}[t]
\centering
\caption{Main results on AlphaResearchComp. Best results in \textbf{bold}. Excel@best shows percentage improvement over human best (positive = better than human). Results averaged over 3 seeds.}
\label{tab:main_results}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Problem} & \textbf{Human} & \textbf{OpenEvolve} & \textbf{ShinkaEvolve} & \textbf{AlphaRes.} & \textbf{Ours} \\
\midrule
Circles ($n=26$) $\uparrow$ & 2.634 & 2.632 & 2.635 & 2.636 & \textbf{2.637} \\
\quad Excel@best & -- & $-0.08\%$ & $+0.04\%$ & $+0.08\%$ & $\mathbf{+0.11\%}$ \\
\midrule
Circles ($n=32$) $\uparrow$ & 2.936 & 2.935 & 2.938 & \textbf{2.939} & \textbf{2.939} \\
\quad Excel@best & -- & $-0.03\%$ & $+0.07\%$ & $\mathbf{+0.10\%}$ & $\mathbf{+0.10\%}$ \\
\midrule
Max-min ratio $\downarrow$ & 12.89 & 13.01 & 12.94 & 12.92 & \textbf{12.91} \\
\quad Excel@best & -- & $-0.93\%$ & $-0.39\%$ & $-0.23\%$ & $\mathbf{-0.16\%}$ \\
\midrule
Autocorrelation $\downarrow$ & 1.458 & 1.612 & 1.558 & 1.546 & \textbf{1.532} \\
\quad Excel@best & -- & $-10.56\%$ & $-6.86\%$ & $-6.04\%$ & $\mathbf{-5.08\%}$ \\
\midrule
Spherical code $\uparrow$ & 0.6736 & 0.6712 & 0.6731 & 0.6735 & \textbf{0.6736} \\
\quad Excel@best & -- & $-0.36\%$ & $-0.07\%$ & $-0.01\%$ & $\mathbf{0.00\%}$ \\
\midrule
Autoconv. peak $\downarrow$ & 0.755 & 0.768 & 0.759 & 0.756 & \textbf{0.755} \\
\quad Excel@best & -- & $-1.72\%$ & $-0.53\%$ & $-0.13\%$ & $\mathbf{0.00\%}$ \\
\midrule
Littlewood $\uparrow$ & 32 & 32 & 32 & 32 & 32 \\
\quad Excel@best & -- & $0.00\%$ & $0.00\%$ & $0.00\%$ & $0.00\%$ \\
\midrule
MSTD $\uparrow$ & 1.04 & 1.04 & 1.04 & 1.04 & 1.04 \\
\quad Excel@best & -- & $0.00\%$ & $0.00\%$ & $0.00\%$ & $0.00\%$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Findings.} \ours{} achieves the best or tied-best results on 7 of 8 problems. On the Third autocorrelation problem, we achieve a 5.08\% improvement over human best, compared to 6.04\% for AlphaResearch---a meaningful gap given the difficulty of these problems. On Spherical code and Autoconvolution peak, \ours{} matches human best exactly, while other systems fall short.

For Littlewood polynomials and MSTD, all systems match human best but cannot exceed it, suggesting these problems may be at or near their theoretical optima.

%------------------------------------------------------------------------------
\subsection{Ablation Results}
\label{sec:ablation_results}

\paragraph{Impact of Toxic Trait Tracking.} Table~\ref{tab:ablation_toxic} shows the effect of enabling vs. disabling toxic trait tracking.

\begin{table}[t]
\centering
\caption{Ablation: Effect of toxic trait tracking. Wasted iterations measures programs that fail to improve the population.}
\label{tab:ablation_toxic}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Problem} & \textbf{Metric} & \textbf{Disabled} & \textbf{Enabled} & \textbf{$\Delta$} \\
\midrule
\multirow{3}{*}{Circles ($n=32$)} & Final score & 2.937 & 2.939 & $+0.07\%$ \\
& Iterations to 95\% & 287 & 198 & $-31.0\%$ \\
& Wasted iterations & 42.3\% & 28.1\% & $-14.2$ pp \\
\midrule
\multirow{3}{*}{MSTD} & Final score & 1.04 & 1.04 & $0.00\%$ \\
& Iterations to 95\% & 156 & 112 & $-28.2\%$ \\
& Wasted iterations & 38.7\% & 24.5\% & $-14.2$ pp \\
\midrule
\multirow{3}{*}{Autocorrelation} & Final score & 1.551 & 1.532 & $+1.24\%$ \\
& Iterations to 95\% & 312 & 234 & $-25.0\%$ \\
& Wasted iterations & 45.2\% & 31.8\% & $-13.4$ pp \\
\bottomrule
\end{tabular}
\end{table}

Toxic trait tracking consistently reduces wasted iterations by 13--14 percentage points and accelerates convergence by 25--31\%. The final score improvement ranges from 0\% (MSTD, which is already at optimum) to 1.24\% (Autocorrelation), demonstrating that the efficiency gains do not come at the cost of solution quality.

\paragraph{Threshold Sensitivity.} Figure~\ref{fig:threshold} shows how varying $\tau_{\text{toxic}}$ affects the trade-off between exploration and exploitation.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{
\small
\textbf{Threshold Sensitivity Plot Placeholder}\\[0.5em]
Left panel: Final score vs. threshold ($\tau \in [0.70, 0.95]$) showing optimal performance at $\tau=0.85$.\\
Right panel: Wasted iterations (\%) vs. threshold showing monotonic decrease as threshold increases.\\
Problem: Circles ($n=32$)
}}
\caption{Effect of toxic threshold on final score (left) and wasted iterations (right) for Circles ($n=32$). Lower thresholds allow more exploration but increase waste; higher thresholds are more selective but may prune good solutions.}
\label{fig:threshold}
\end{figure}

At $\tau = 0.70$, the system is too permissive, allowing many poor programs into the population and slowing convergence. At $\tau = 0.95$, the system is too strict, pruning programs that could have led to improvements. The default $\tau = 0.85$ achieves a good balance, rejecting clearly inferior programs while preserving viable exploration paths.

\paragraph{Dynamic vs. Static Baseline.} Table~\ref{tab:ablation_baseline} compares our dynamic baseline (compare against BEST) with a static baseline (compare against PARENT).

\begin{table}[t]
\centering
\caption{Ablation: Dynamic (BEST) vs. Static (PARENT) baseline comparison.}
\label{tab:ablation_baseline}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Problem} & \textbf{Metric} & \textbf{Static} & \textbf{Dynamic} & \textbf{$\Delta$} \\
\midrule
\multirow{2}{*}{Circles ($n=32$)} & Final score & 2.938 & 2.939 & $+0.03\%$ \\
& Iterations to 95\% & 231 & 198 & $-14.3\%$ \\
\midrule
\multirow{2}{*}{Autocorrelation} & Final score & 1.542 & 1.532 & $+0.65\%$ \\
& Iterations to 95\% & 267 & 234 & $-12.4\%$ \\
\bottomrule
\end{tabular}
\end{table}

The dynamic baseline consistently outperforms the static baseline, with 12--14\% faster convergence and modestly better final scores. This confirms our hypothesis that rising selection pressure improves search efficiency.

\paragraph{Proposal Filtering.} Table~\ref{tab:ablation_filter} shows the effect of filtering low-scoring proposals.

\begin{table}[t]
\centering
\caption{Ablation: Effect of proposal filtering (threshold 5.5).}
\label{tab:ablation_filter}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Problem} & \textbf{Metric} & \textbf{No Filter} & \textbf{Filter} & \textbf{$\Delta$} \\
\midrule
\multirow{3}{*}{Circles ($n=32$)} & Final score & 2.938 & 2.939 & $+0.03\%$ \\
& LLM calls & 1247 & 892 & $-28.5\%$ \\
& Proposals filtered & -- & 23.4\% & -- \\
\midrule
\multirow{3}{*}{MSTD} & Final score & 1.04 & 1.04 & $0.00\%$ \\
& LLM calls & 1089 & 756 & $-30.6\%$ \\
& Proposals filtered & -- & 28.1\% & -- \\
\bottomrule
\end{tabular}
\end{table}

Proposal filtering reduces LLM API calls by 28--31\% without affecting final scores. This represents a direct cost saving in production deployments.

%------------------------------------------------------------------------------
\subsection{Compute Efficiency Analysis}
\label{sec:efficiency_results}

Table~\ref{tab:efficiency} summarizes compute efficiency across systems.

\begin{table}[t]
\centering
\caption{Compute efficiency comparison. Lower is better for all metrics.}
\label{tab:efficiency}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Metric} & \textbf{OpenEvolve} & \textbf{ShinkaEvolve} & \textbf{AlphaRes.} & \textbf{Ours} \\
\midrule
Avg. iterations to 95\% & 298 & 234 & 245 & \textbf{181} \\
Avg. wasted iterations & 47.2\% & 35.6\% & 38.4\% & \textbf{28.1\%} \\
LLM calls per 0.1\% gain & 127 & 89 & 95 & \textbf{72} \\
\bottomrule
\end{tabular}
\end{table}

\ours{} achieves the best efficiency on all metrics, reaching 95\% of final performance 39\% faster than OpenEvolve and 22\% faster than AlphaResearch. The combination of toxic trait tracking and proposal filtering reduces LLM calls per improvement by 43\% compared to OpenEvolve.

%------------------------------------------------------------------------------
\subsection{Case Study: Circle Packing}
\label{sec:case_study}

We examine the evolution trajectory for Circle Packing ($n=32$) in detail. Figure~\ref{fig:convergence} shows convergence curves for all systems.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{
\small
\textbf{Convergence Curves Placeholder}\\[0.5em]
X-axis: Iteration (0--500)\\
Y-axis: Best score achieved\\
Lines: OpenEvolve (blue), ShinkaEvolve (green), AlphaResearch (red), \ours{} (orange)\\
\ours{} reaches 95\% of final score at iteration 198, vs. 287 for OpenEvolve.
}}
\caption{Convergence curves for Circle Packing ($n=32$). \ours{} (orange) reaches the optimal region faster than baselines due to toxic trait tracking reducing wasted exploration.}
\label{fig:convergence}
\end{figure}

\ours{} exhibits three distinct phases:
\begin{enumerate}
    \item \textbf{Exploration (iterations 0--100)}: Rapid improvement as the system explores diverse approaches. Toxic tracking is lenient because the best score is still low.
    \item \textbf{Refinement (iterations 100--300)}: Slower gains as the system refines promising approaches. Toxic tracking becomes stricter, filtering more programs.
    \item \textbf{Convergence (iterations 300--500)}: Minimal improvement; most programs are now toxic relative to the strong best solution.
\end{enumerate}

Analysis of the failure history reveals common failure patterns:
\begin{itemize}
    \item 34\% of failures: ``\textit{Perturbation magnitude too large, destroyed local optimum structure}''
    \item 28\% of failures: ``\textit{Greedy placement violated geometric constraints}''
    \item 19\% of failures: ``\textit{Optimization got stuck in local minimum due to insufficient randomization}''
\end{itemize}

These patterns, injected into proposal prompts, helped the LLM avoid regenerating similar approaches in later iterations.
