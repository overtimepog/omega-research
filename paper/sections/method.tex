% Method Section
% Core technical content describing Omega Evolve

\ours{} is an evolutionary code optimization framework that combines LLM-guided program mutation with four novel mechanisms for improving search efficiency: toxic trait tracking, peer-review reward decoupling, failure-driven learning, and automated bug fixing. Figure~\ref{fig:system_overview} provides an overview of the system architecture.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{
\small
\textbf{System Architecture Placeholder}\\[0.5em]
The \ours{} pipeline: (1) Sample parent + inspirations from database (excluding toxic programs) $\rightarrow$ (2) Generate proposal with failure history context $\rightarrow$ (3) Score proposal with reward model (filter if $<5.5$) $\rightarrow$ (4) Generate code mutation $\rightarrow$ (5) Evaluate program $\rightarrow$ (6) If error: attempt bug fix (up to 3 tries) $\rightarrow$ (7) Toxic check against BEST $\rightarrow$ (8) If toxic: record failure; else: add to database via MAP-Elites.
}}
\caption{Overview of the \ours{} system architecture. The pipeline integrates proposal filtering, toxic trait tracking, failure-driven learning, and automated bug fixing into the evolutionary loop. All LLM operations use OpenRouter API (GPT-5.1-Codex-Mini for code generation, Gemini-2.5-Flash-Lite for reward scoring).}
\label{fig:system_overview}
\end{figure}

%------------------------------------------------------------------------------
\subsection{System Overview}
\label{sec:system_overview}

The \ours{} pipeline operates through an iterative evolution loop. At each iteration $t$:

\begin{enumerate}
    \item \textbf{Sampling}: Select a parent program $p$ and inspiration programs $\{i_1, \ldots, i_k\}$ from the population database, excluding programs marked as \toxic{}.
    \item \textbf{Proposal Generation}: Generate a research proposal $r_t$ using an LLM, conditioned on $p$, the inspirations, and the failure history.
    \item \textbf{Proposal Scoring}: Score $r_t$ using the peer-review reward model. If $\text{score}(r_t) < \tau_{\text{proposal}}$, skip to the next iteration.
    \item \textbf{Code Generation}: Generate child program $c$ by mutating $p$ according to proposal $r_t$.
    \item \textbf{Evaluation}: Execute $c$ on the benchmark evaluator to obtain metrics $m_c$.
    \item \textbf{Toxic Check}: Compare $c$ against the current best program $b^*$. If $\frac{m_c}{m_{b^*}} < \tau_{\text{toxic}}$, mark $c$ as \toxic{} and record the failure.
    \item \textbf{Database Update}: If not \toxic{}, add $c$ to the population database using MAP-Elites.
\end{enumerate}

This loop continues until a budget of $T$ iterations is exhausted or convergence criteria are met.

%------------------------------------------------------------------------------
\subsection{Peer-Review Reward Model}
\label{sec:reward_model}

A key insight of \ours{} is that research quality and code quality are distinct signals that should be evaluated independently. We introduce a \textbf{peer-review reward model} that scores research proposals before any code is generated, enabling early filtering of low-quality ideas.

\paragraph{Training Data.} We fine-tune our reward model on peer review data from ICLR 2017--2024, comprising 24,445 papers with their associated review scores. For each paper, we extract the abstract as input and use the average review score (1--10 scale) as the target. The training set uses papers from 2017--2023, with 2024 papers held out for validation. We further evaluate on 100 ICLR 2025 papers as a test set to assess temporal generalization.

\paragraph{Model Architecture.} We fine-tune Qwen2.5-7B-Instruct~\cite{qwen2.5} on the review prediction task. The model receives the proposal text and outputs a JSON response containing a numerical score and brief explanation:
\begin{verbatim}
{"score": 7.2, "explanation": "Novel approach with clear methodology..."}
\end{verbatim}

\paragraph{Integration.} During evolution, each generated proposal $r_t$ is scored by the reward model before code generation. We apply a threshold $\tau_{\text{proposal}} = 5.5$ (corresponding to the typical accept/reject boundary at ICLR):
\begin{equation}
    \text{generate\_code}(r_t) =
    \begin{cases}
        \text{True} & \text{if } \text{score}(r_t) \geq \tau_{\text{proposal}} \\
        \text{False} & \text{otherwise}
    \end{cases}
\end{equation}

This filtering step eliminates low-quality proposals before expensive LLM calls for code generation, reducing API costs and focusing search on promising directions.

\paragraph{Evaluation.} On the ICLR 2025 holdout set, our reward model achieves 72\% accuracy on binary accept/reject prediction (threshold 5.5), compared to 37\% for the base Qwen2.5-7B-Instruct model and 53\% for GPT-4. Notably, even human reviewers achieve only 65\% agreement on this task, suggesting our model captures meaningful aspects of research quality.

%------------------------------------------------------------------------------
\subsection{Toxic Trait Tracking}
\label{sec:toxic_traits}

The central contribution of \ours{} is \textbf{Toxic Trait Tracking}, a negative selection mechanism that identifies and excludes underperforming programs from the breeding population. Unlike traditional evolutionary algorithms that simply discard poor solutions, we explicitly mark them as \toxic{} and leverage their failure patterns to improve future generations.

\paragraph{Dynamic Baseline Comparison.} A critical design decision is \emph{what to compare against} when determining if a program is \toxic{}. Prior work typically compares child performance against the immediate parent. We argue this is suboptimal because it creates a static threshold that doesn't adapt as evolution progresses.

Instead, \ours{} compares each child program against the \textbf{current best program} $b^*$ in the population:
\begin{equation}
    \text{is\_toxic}(c) = \frac{m_c}{m_{b^*}} < \tau_{\text{toxic}}
    \label{eq:toxic}
\end{equation}
where $m_c$ is the metric value for child $c$, $m_{b^*}$ is the metric for the current best, and $\tau_{\text{toxic}} = 0.85$ by default.

This dynamic baseline creates \textbf{rising selection pressure}: as better solutions are discovered, the threshold for acceptability automatically increases. Early in evolution, programs achieving 85\% of a weak baseline may be acceptable; later, they must achieve 85\% of a much stronger solution.

\paragraph{Algorithm.} Algorithm~\ref{alg:toxic_tracking} presents the toxic trait tracking procedure.

\begin{algorithm}[t]
\caption{Toxic Trait Tracking}
\label{alg:toxic_tracking}
\begin{algorithmic}[1]
\REQUIRE Child program $c$, best program $b^*$, threshold $\tau$, failure tracker $F$
\STATE $m_c \gets \text{evaluate}(c)$
\STATE $m_{b^*} \gets \text{metrics}(b^*$)
\STATE $\text{ratio} \gets m_c / m_{b^*}$
\IF{$\text{ratio} < \tau$}
    \STATE $\text{reason} \gets \text{LLM\_analyze\_failure}(c, b^*)$
    \STATE $F.\text{add}(c.\text{id}, c.\text{code}, m_c, \text{reason})$
    \STATE $\text{toxic\_set}.\text{add}(c.\text{id})$
    \RETURN \textbf{True} \COMMENT{Program is toxic}
\ELSE
    \RETURN \textbf{False} \COMMENT{Program is viable}
\ENDIF
\end{algorithmic}
\end{algorithm}

\paragraph{Efficient Implementation.} To avoid performance overhead during the sampling hot path, we maintain the set of \toxic{} program IDs in memory, enabling $O(1)$ lookup. The detailed failure records (code, metrics, analysis) are stored persistently in JSON format and loaded lazily.

\paragraph{Exclusion from Breeding.} Programs marked as \toxic{} are excluded from both parent selection and inspiration sampling:
\begin{equation}
    \text{candidates} = \{p \in \text{population} : p.\text{id} \notin \text{toxic\_set}\}
\end{equation}

This prevents the system from building upon failed approaches, focusing search on viable regions of the solution space.

%------------------------------------------------------------------------------
\subsection{Failure-Driven Learning}
\label{sec:failure_learning}

Beyond simply excluding \toxic{} programs, \ours{} \textbf{learns from failures} by recording detailed failure information and injecting it into future proposal generation prompts.

\paragraph{Failure Records.} When a program is marked \toxic{}, we record:
\begin{itemize}
    \item Program ID and parent ID
    \item Full code (child and parent)
    \item Metrics comparison (child vs. best)
    \item Performance ratio
    \item Proposal summary that led to the failure
    \item LLM-generated root cause analysis
\end{itemize}

\paragraph{Root Cause Analysis.} We prompt an LLM to analyze why the modification failed:

\begin{quote}
\small
\texttt{Given the parent code achieving score X and child code achieving score Y (ratio: Z\%), analyze why this modification failed. Provide a 1--2 sentence technical explanation of the root cause.}
\end{quote}

Example outputs include: ``\textit{Loop interchange broke memory locality; cache-unfriendly access pattern increased memory latency}'' or ``\textit{Parallel prefix sum introduced race condition in accumulator variable.}''

\paragraph{Prompt Injection.} The most recent $k$ failure records (default $k=10$) are included in the proposal generation prompt:

\begin{quote}
\small
\texttt{=== Previously Failed Approaches (avoid repeating) ===} \\
\texttt{1. "Improved sorting with memory locality" -> Failed: Loop interchange broke cache coherence (achieved 72\% of best)} \\
\texttt{2. "Parallel prefix sum optimization" -> Failed: Race condition in accumulator (achieved 68\% of best)} \\
\texttt{...}
\end{quote}

This contextual guidance helps the LLM avoid regenerating similar failed approaches, accelerating convergence by preventing repetition of known mistakes.

%------------------------------------------------------------------------------
\subsection{Population Management}
\label{sec:population}

\ours{} employs a combination of MAP-Elites~\cite{mapelites} and island-based evolution for population management.

\paragraph{MAP-Elites Feature Map.} We discretize the solution space into a 2D grid based on two behavioral descriptors:
\begin{itemize}
    \item \textbf{Score dimension}: The primary evaluation metric, binned into 10 levels.
    \item \textbf{Complexity dimension}: Code complexity (line count), binned into 10 levels.
\end{itemize}

Each cell $(i, j)$ in the grid stores at most one program---the elite for that region. When a new program maps to an occupied cell, it replaces the incumbent only if its score is higher. This quality-diversity mechanism maintains exploration across different program architectures while optimizing within each niche.

\paragraph{Island-Based Evolution.} We maintain $N_{\text{islands}} = 5$ parallel subpopulations with periodic migration. Every $I_{\text{migrate}} = 50$ iterations, 10\% of programs from each island migrate to neighboring islands. This prevents premature convergence and maintains genetic diversity across the search.

\paragraph{Integration with Toxic Tracking.} The toxic trait mechanism operates orthogonally to MAP-Elites and island evolution. Toxic programs are excluded from sampling regardless of their position in the feature map or island membership. This ensures that even if a \toxic{} program would be an elite for its feature cell, it cannot propagate its traits to future generations.

%------------------------------------------------------------------------------
\subsection{Automated Bug Fixing}
\label{sec:bug_fixer}

When program evaluation returns error metrics (syntax errors, runtime exceptions, or invalid outputs), \ours{} employs an \textbf{automated bug fixer loop} rather than immediately discarding the program.

\paragraph{Bug Fixing Pipeline.} The bug fixer operates as follows:
\begin{enumerate}
    \item \textbf{Error Detection}: If evaluation returns error metrics, extract error type and traceback.
    \item \textbf{Fix Attempt}: Prompt the LLM with the buggy code, error message, and original proposal to generate a fix.
    \item \textbf{Strategy Selection}: Use diff-based fixes for the first 5 consecutive failures; fall back to full rewrite if diffs fail repeatedly.
    \item \textbf{Re-evaluation}: Test the fixed code; if still failing, retry up to 3 times with accumulated error context.
    \item \textbf{Graceful Degradation}: If all fix attempts fail, skip the iteration rather than adding broken code.
\end{enumerate}

This mechanism recovers approximately 60--70\% of programs that would otherwise be discarded due to bugs, significantly improving the yield of each LLM generation call.

%------------------------------------------------------------------------------
\subsection{OpenRouter-Based Architecture}
\label{sec:openrouter}

Unlike the original AlphaResearch system which uses local vLLM inference for the reward model, \ours{} is built entirely on the \textbf{OpenRouter API}. This architectural choice has several implications:

\paragraph{Unified API Interface.} All LLM operations---proposal generation, code mutation, reward scoring, failure analysis, and bug fixing---use the same OpenRouter endpoint. This simplifies deployment and eliminates the need for local GPU resources dedicated to reward model inference.

\paragraph{Model Selection.} We use different models optimized for different tasks:
\begin{itemize}
    \item \textbf{Code Generation}: GPT-5.1-Codex-Mini via OpenRouter---optimized for code tasks, strong at diff-based mutations and bug fixing.
    \item \textbf{Reward Scoring}: Gemini-2.5-Flash-Lite via OpenRouter---fast inference and cost-effective for high-volume proposal scoring.
\end{itemize}
This separation allows us to optimize for both quality (code generation) and cost-efficiency (reward scoring), as proposal scoring happens more frequently than code generation.

\paragraph{Cost-Performance Trade-off.} API-based inference trades local compute costs for API costs. However, this is offset by: (1) proposal filtering reducing total API calls by 25--30\%, (2) toxic tracking reducing wasted iterations, and (3) bug fixing improving per-call yield. The net effect is competitive cost-efficiency with improved accessibility.
