# EvolveAgent Configuration - OpenRouter Only
# Uses OpenRouter API for all LLM operations

# General settings
max_iterations: 1000
checkpoint_interval: 50
log_level: "INFO"
log_dir: null
best_solution_dir: results/best_solution  # Directory to auto-save best solution (default: output_dir/best_solution)
random_seed: null

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false
max_code_length: 100000
max_diff_generation_retries: 5        # Maximum retries for LLM diff generation failures

# Changes documentation settings
generate_changes_doc: true            # Auto-generate changes.md explaining code evolution
changes_doc_max_retries: 5            # Maximum retries for changes documentation LLM generation

toxic_trait:
    enabled: true
    threshold: 0.85  # Child must score â‰¥85% of parent
    comparison_metric: "combined_score"
    failure_history_path: null  # Defaults to {db_path}/failures/
    max_failures_in_prompt: 10  # Limit LLM context size


# LLM configuration - OpenRouter API
llm:
  # Models for evolution (using OpenRouter)
  models:
    - name: "openai/gpt-5.1-codex-mini"
      weight: 1.0
    # Alternative OpenRouter models:
    # - name: "anthropic/claude-3.5-sonnet"
    #   weight: 0.5
    # - name: "google/gemini-2.0-flash-exp"
    #   weight: 0.3
    # - name: "qwen/qwen-2.5-72b-instruct"
    #   weight: 0.2

  # Models for evaluation (using OpenRouter)
  evaluator_models:
    - name: "openai/gpt-5.1-codex-mini"
      weight: 1.0

  # API configuration - OpenRouter
  api_base: "https://openrouter.ai/api/v1"
  api_key: "${OPENROUTER_API_KEY}"  # Set via environment variable

  # Generation parameters
  temperature: 0.7
  top_p: 0.95
  max_tokens: 8192

  # Request parameters
  timeout: 300
  retries: 5
  retry_delay: 5

# Prompt configuration
prompt:
  template_dir: null
  num_top_programs: 3
  num_diverse_programs: 2
  use_template_stochasticity: true
  template_variations:
    improvement_suggestion:
      - "Here's how we could improve this code:"
      - "I suggest the following improvements:"
      - "We can enhance this code by:"

# Database configuration
database:
  db_path: null
  in_memory: true
  log_prompts: true
  population_size: 1000
  archive_size: 100
  num_islands: 5
  migration_interval: 50
  migration_rate: 0.1
  elite_selection_ratio: 0.1
  exploration_ratio: 0.2
  exploitation_ratio: 0.7

  # Dynamic quality thresholds for parent selection
  parent_selection_percentile: 0.5      # Only top 50% of programs can be parents
  archive_selection_percentile: 0.7     # Only top 30% can enter archive
  min_score_vs_baseline: 0.5            # Must be at least 50% as good as initial program
  enable_quality_filtering: true        # Enable dynamic quality-based filtering

  feature_dimensions:
    - "score"
    - "complexity"
  feature_bins: 10

# Evaluator configuration
evaluator:
  timeout: 300
  max_retries: 3

  # Self-healing / bug fixing settings
  enable_bug_fixer: true  # Enable LLM-based automatic bug fixing
  enable_pre_validation: true  # Enable pre-evaluation validation checks
  max_fix_attempts: 20  # Maximum number of bug fix attempts (increases retry capability)

  cascade_evaluation: false
  cascade_thresholds:
    - 0.5
    - 0.75
    - 0.9
  parallel_evaluations: 4
  use_llm_feedback: false
  llm_feedback_weight: 0.1

# Reward model configuration - OpenRouter API
rewardmodel:
  model_type: api
  model_name: "google/gemini-2.5-flash-lite"

  # Recommended models for LLM-as-judge scoring (November 2025):
  # Best for structured evaluation & summarization:
  #   - "google/gemini-2.5-flash" (fast, cost-effective, excellent JSON output)
  #   - "anthropic/claude-sonnet-4.5" (superior instruction following, reliable)
  #   - "openai/gpt-5.1" (best-in-class accuracy, premium pricing)
  #   - "qwen/qwen-2.5-7b-instruct" (fast, good for summaries)
  #
  # Reasoning-focused (NOTE: "thinking" models may not work well for summarization):
  #   - "openrouter/sherlock-think-alpha" (logical scoring, step-by-step reasoning)
  #   - "moonshotai/kimi-k2-thinking" (multi-step reasoning, explicit logic)
  #
  # Budget-friendly:
  #   - "minimax/m2" (cost-effective, reliable basic scoring)
  #   - "google/gemini-2.5-flash" (best speed-cost ratio)

  # API configuration - OpenRouter
  base_url: "https://openrouter.ai/api/v1"
  api_key: "${OPENROUTER_API_KEY}"  # Set via environment variable

  # Generation parameters
  temperature: 0.3  # Lower temp (0.2-0.3) for consistent scoring (2025 best practice)
  top_p: 0.95
  max_tokens: 8192

  # Scoring threshold
  proposal_score_threshold: 5.5

  # Request parameters
  jsonl_file: "results/reward_results.jsonl"
  max_retries: 5
  retry_delay: 5

  # Proposal summarization for console display (enabled by default)
  # NOTE: "Thinking" models may return empty responses for summarization
  # For best results, use non-thinking models like gemini-2.5-flash or qwen-2.5-7b-instruct
  enable_summarization: true  # Generate concise summaries for console logs
  summary_max_tokens: 150  # Token limit for summaries (shorter = faster/cheaper)

# Recommended OpenRouter models for evolution:
# Fast & Cost-Effective:
#   - deepseek/deepseek-chat (very cost effective, good for iteration)
#   - anthropic/claude-3-haiku (fast, balanced)
#   - google/gemini-2.0-flash-lite (good speed/quality)
#
# High Quality:
#   - anthropic/claude-3.5-sonnet (excellent reasoning)
#   - google/gemini-2.0-flash-exp (experimental, powerful)
#   - qwen/qwen-2.5-72b-instruct (strong open-source)
#
# Research-Focused:
#   - meta-llama/llama-3.1-70b-instruct (open research)
#   - mistralai/mistral-large (good balance)
